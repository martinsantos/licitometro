# Implementaci√≥n Gran Scraper v2.0

## Resumen de Cambios

Este documento describe la implementaci√≥n del sistema mejorado de scraping para LICITOMETRO, incluyendo scheduling autom√°tico, URLs can√≥nicas y deduplicaci√≥n.

---

## üöÄ Nuevas Funcionalidades

### 1. Sistema de Scheduling (APScheduler)

**Archivos creados:**
- `backend/services/scheduler_service.py` - Servicio de scheduling
- `backend/routers/scheduler.py` - Endpoints API para control del scheduler
- `backend/models/scraper_run.py` - Modelo para tracking de ejecuciones

**Caracter√≠sticas:**
- Ejecuci√≥n autom√°tica seg√∫n cron schedule de cada scraper
- Tracking detallado de cada ejecuci√≥n (items_found, items_saved, errors, etc.)
- Reintentos autom√°ticos con backoff
- API para control manual (`/api/scheduler/trigger/{scraper_name}`)
- Inicializaci√≥n autom√°tica al iniciar el servidor

**Endpoints API:**
```
POST   /api/scheduler/start              # Iniciar scheduler
POST   /api/scheduler/stop               # Detener scheduler
GET    /api/scheduler/status             # Estado actual
GET    /api/scheduler/jobs               # Jobs programados
POST   /api/scheduler/trigger/{name}     # Ejecutar manualmente
GET    /api/scheduler/runs               # Historial de ejecuciones
GET    /api/scheduler/runs/{id}          # Detalle de ejecuci√≥n
GET    /api/scheduler/runs/{id}/logs     # Logs de ejecuci√≥n
GET    /api/scheduler/stats              # Estad√≠sticas agregadas
```

### 2. URLs Can√≥nicas √önicas

**Archivos creados:**
- `backend/services/url_resolver.py` - Resoluci√≥n de URLs

**Modelo actualizado:**
- `backend/models/licitacion.py` - Nuevos campos: `canonical_url`, `source_urls`, `url_quality`

**Scraper actualizado:**
- `backend/scrapers/mendoza_compra.py` - Genera URLs can√≥nicas y content_hash

**Jerarqu√≠a de URL Quality:**
1. **direct** - URL va directamente a la p√°gina del proceso (VistaPreviaPliego)
2. **proxy** - URL usa proxy/form submission (/api/comprar/proceso/open)
3. **partial** - URL solo va a la lista (Compras.aspx)

**Endpoints API:**
```
GET    /api/licitaciones/{id}/redirect   # Redirecci√≥n a URL can√≥nica
GET    /api/licitaciones/{id}/urls       # Todas las URLs disponibles
POST   /api/licitaciones/{id}/resolve-url # Resolver URL espec√≠fica
GET    /api/licitaciones/stats/url-quality # Estad√≠sticas de calidad de URLs
```

### 3. Sistema de Deduplicaci√≥n

**Archivos creados:**
- `backend/services/deduplication_service.py` - Servicio de deduplicaci√≥n

**Algoritmo de matching:**
1. **Mismo n√∫mero de expediente** ‚Üí Match exacto
2. **Mismo n√∫mero de licitaci√≥n** ‚Üí Match exacto
3. **Content hash igual** ‚Üí Match exacto
4. **Similaridad fuzzy > 85%** + misma organizaci√≥n + fechas cercanas ‚Üí Match

**Estrategia de merge:**
- Conservar datos m√°s completos
- Fusionar URLs de todas las fuentes
- Mantener historial de merges
- Marcar registro como `is_merged = true`

**Endpoints API:**
```
POST   /api/licitaciones/deduplicate?jurisdiccion=Mendoza
```

### 4. Dependencias Agregadas

```
apscheduler>=3.10.4
python-crontab>=1.2.0
fuzzywuzzy>=0.18.0
python-levenshtein>=0.21.0
```

---

## üìÅ Estructura de Archivos

```
backend/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Exporta ScraperRun
‚îÇ   ‚îú‚îÄ‚îÄ licitacion.py            # Nuevos campos: canonical_url, source_urls, url_quality, content_hash
‚îÇ   ‚îú‚îÄ‚îÄ scraper_config.py        # (existente)
‚îÇ   ‚îî‚îÄ‚îÄ scraper_run.py           # NUEVO - Modelo de ejecuci√≥n
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Exporta servicios
‚îÇ   ‚îú‚îÄ‚îÄ scheduler_service.py     # NUEVO - Scheduling con APScheduler
‚îÇ   ‚îú‚îÄ‚îÄ deduplication_service.py # NUEVO - Deduplicaci√≥n de licitaciones
‚îÇ   ‚îî‚îÄ‚îÄ url_resolver.py          # NUEVO - Resoluci√≥n de URLs can√≥nicas
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py             # NUEVO - Endpoints de scheduling
‚îÇ   ‚îú‚îÄ‚îÄ licitaciones.py          # ACTUALIZADO - Endpoints de URL/deduplicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ scraper_configs.py       # (existente)
‚îÇ   ‚îî‚îÄ‚îÄ comprar.py               # (existente)
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ mendoza_compra.py        # ACTUALIZADO - URLs can√≥nicas, content_hash
‚îÇ   ‚îî‚îÄ‚îÄ ...                      # (otros scrapers)
‚îî‚îÄ‚îÄ server.py                    # ACTUALIZADO - Auto-inicio de scheduler
```

---

## üîß Configuraci√≥n

### Configuraci√≥n de Scraper (ejemplo actualizado)

```json
{
  "name": "COMPR.AR Mendoza",
  "url": "https://comprar.mendoza.gov.ar/",
  "active": true,
  "schedule": "0 7,13,19 * * 1-5",
  "selectors": {
    "use_selenium_pliego": true,
    "selenium_max_pages": 9,
    "disable_date_filter": true
  },
  "pagination": {
    "list_urls": [
      "https://comprar.mendoza.gov.ar/Compras.aspx?qs=W1HXHGHtH10="
    ]
  }
}
```

### Variables de Entorno

```bash
# URL base para proxies (debe ser accesible p√∫blicamente)
API_BASE_URL=http://localhost:8001

# MongoDB
MONGO_URL=mongodb://localhost:27017
DB_NAME=licitaciones_db
```

---

## üìä M√©tricas Esperadas

| M√©trica | Antes | Despu√©s (Objetivo) |
|---------|-------|-------------------|
| Ejecuci√≥n autom√°tica | Manual | Cada 4 horas (cron) |
| Tracking de ejecuciones | No | Completo con logs |
| URLs directas (PLIEGO) | ~27% | >80% |
| Deduplicaci√≥n | Manual | Autom√°tica |
| Calidad de datos | Variable | Consistente |

---

## üîÑ Flujo de Datos

```
1. Scheduler ejecuta seg√∫n cron
   ‚Üì
2. Scraper extrae datos + genera URLs can√≥nicas + content_hash
   ‚Üì
3. Datos guardados en MongoDB
   ‚Üì
4. Deduplicaci√≥n autom√°tica (opcional/configurable)
   ‚Üì
5. Frontend consume API con URLs can√≥nicas
```

---

## üöß Pr√≥ximos Pasos

1. **Testing** - Verificar funcionamiento del scheduler
2. **Mejorar captura de URLs PLIEGO** - Optimizar Selenium
3. **Agregar m√°s scrapers** - AYSAM, OSEP, UNCuyo
4. **Dashboard de monitoreo** - UI para ver ejecuciones
5. **Alertas** - Notificaciones cuando scrapers fallan
6. **Escalar a otras provincias** - Buenos Aires, CABA, C√≥rdoba, Santa Fe

---

## üìù Comandos √ötiles

```bash
# Instalar dependencias
pip install -r backend/requirements.txt

# Iniciar scheduler manualmente (ya se inicia autom√°ticamente)
curl -X POST http://localhost:8001/api/scheduler/start

# Ejecutar scraper manualmente
curl -X POST http://localhost:8001/api/scheduler/trigger/COMPR.AR%20Mendoza

# Ver estado del scheduler
curl http://localhost:8001/api/scheduler/status

# Ver √∫ltimas ejecuciones
curl http://localhost:8001/api/scheduler/runs

# Ejecutar deduplicaci√≥n
curl -X POST "http://localhost:8001/api/licitaciones/deduplicate?jurisdiccion=Mendoza"

# Estad√≠sticas de URLs
curl http://localhost:8001/api/licitaciones/stats/url-quality
```

---

## ‚ö†Ô∏è Notas Importantes

1. **Selenium** requiere Chrome/ChromeDriver instalado
2. **Scheduler** se inicia autom√°ticamente con el servidor
3. **Deduplicaci√≥n** debe ejecutarse manualmente o programarse por separado
4. **URLs proxy** requieren que `API_BASE_URL` sea accesible desde el navegador del usuario

---

*Documento creado: 2026-02-05*
*Versi√≥n: 2.0*
