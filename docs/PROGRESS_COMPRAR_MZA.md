# Estado actual: COMPR.AR Mendoza (Scraper + UI)

Fecha: 2026-02-05

---

## ‚úÖ Completado (Fase 1 y 2)

### Scraper COMPR.AR v2.0

- ‚úÖ Scraper COMPR.AR consume el listado oficial en `Compras.aspx?qs=W1HXHGHtH10=`.
- ‚úÖ **NUEVO**: Scraper v2 con cach√© persistente de URLs PLIEGO
- ‚úÖ **NUEVO**: Sistema de scheduling autom√°tico (APScheduler)
- ‚úÖ **NUEVO**: URLs can√≥nicas √∫nicas para cada proceso
- ‚úÖ **NUEVO**: Tracking completo de ejecuciones
- ‚úÖ **NUEVO**: Deduplicaci√≥n autom√°tica

Campos extra√≠dos:
- N√∫mero de proceso
- T√≠tulo
- Tipo de procedimiento
- Fecha/hora de apertura
- Estado (COMPR.AR)
- Unidad ejecutora
- Servicio administrativo/financiero
- Expediente (del PLIEGO)
- Objeto/Descripci√≥n completa
- Moneda
- Lugar de recepci√≥n

### URLs √∫nicas PLIEGO

COMPR.AR no expone URL √∫nica por proceso en el listado; se generan navegando el UI.

Implementaci√≥n mejorada en v2:
- ‚úÖ Cach√© persistente en `storage/pliego_url_cache.json` (TTL 24h)
- ‚úÖ M√∫ltiples estrategias de extracci√≥n de URLs
- ‚úÖ Reintentos con backoff exponencial
- ‚úÖ Mejor manejo de paginaci√≥n en Selenium

Estado actual:
- Se detectan ~23 URLs PLIEGO (de ~85 procesos).
- URLs cacheadas no se recalculan (mejora de performance)
- El resto usa proxy HTML (no tiene URL √∫nica publicada).

### Sistema de Scheduling

- ‚úÖ Scheduler con APScheduler
- ‚úÖ Ejecuci√≥n autom√°tica: 7am, 1pm, 7pm (d√≠as h√°biles)
- ‚úÖ Tracking de ejecuciones en MongoDB (`scraper_runs`)
- ‚úÖ API para ejecuci√≥n manual
- ‚úÖ Estad√≠sticas de ejecuci√≥n

### URLs Can√≥nicas

- ‚úÖ Campo `canonical_url` en modelo Licitacion
- ‚úÖ Campo `source_urls` (dict por fuente)
- ‚úÖ Campo `url_quality`: direct/proxy/partial
- ‚úÖ Redirecci√≥n autom√°tica: `/api/licitaciones/{id}/redirect`

---

## Cambios clave en c√≥digo

### `backend/scrapers/mendoza_compra_v2.py` (NUEVO)
- Scraper mejorado con cach√© de URLs PLIEGO
- Clase `PliegoURLCache` para persistencia
- M√©todo `_collect_pliego_urls_selenium_v2` mejorado
- Estad√≠sticas de ejecuci√≥n
- Generaci√≥n de `content_hash` para deduplicaci√≥n

### `backend/scrapers/mendoza_compra.py` (ACTUALIZADO)
- Campos nuevos: `canonical_url`, `source_urls`, `url_quality`, `content_hash`
- Metadata enriquecida

### `backend/services/scheduler_service.py` (NUEVO)
- Scheduling con APScheduler
- Tracking de ejecuciones
- API de control

### `backend/services/url_resolver.py` (NUEVO)
- Resoluci√≥n de URLs can√≥nicas
- Clasificaci√≥n de calidad de URL

### `backend/services/deduplication_service.py` (NUEVO)
- Deduplicaci√≥n fuzzy matching
- Merge de licitaciones duplicadas

### `backend/routers/scheduler.py` (NUEVO)
- Endpoints para control de scheduler

### `backend/routers/licitaciones.py` (ACTUALIZADO)
- Endpoints de URL: `/redirect`, `/urls`
- Endpoint de deduplicaci√≥n

### `backend/routers/comprar.py`
- `/api/comprar/proceso/open`: auto-post para abrir en COMPR.AR.
- `/api/comprar/proceso/html`: proxy HTML del proceso.

### Frontend
- `frontend/src/pages/LicitacionesPage.js` - Botones "Ver detalle" y "Ir a COMPR.AR".
- `frontend/src/pages/LicitacionDetailPage.js` - Muestra campos adicionales del PLIEGO.

---

## Configuraci√≥n

### Scraper Config

`docs/comprar_mendoza_scraper_config.json`:
```json
{
  "name": "COMPR.AR Mendoza",
  "schedule": "0 7,13,19 * * 1-5",
  "selectors": {
    "use_selenium_pliego": true,
    "selenium_max_pages": 9,
    "disable_date_filter": true,
    "cache_ttl_hours": 24
  }
}
```

### Inicializaci√≥n

```bash
# Cargar configs en MongoDB
python scripts/init_scraper_configs.py

# Verificar estado
curl http://localhost:8001/api/scheduler/status
```

---

## Dependencias / entorno

- Selenium requiere **ChromeDriver** compatible con Chrome.
- APScheduler: `pip install apscheduler`
- fuzzywuzzy: `pip install fuzzywuzzy python-levenshtein`

---

## Resultados actuales

- `storage/comprar_mendoza_run.json` contiene ~85 procesos.
- ~23 con URL PLIEGO real (27%).
- URLs cacheadas en `storage/pliego_url_cache.json`.
- Scheduler ejecutando autom√°ticamente cada 4 horas.

---

## API Endpoints

### Scheduler
```
POST   /api/scheduler/start
POST   /api/scheduler/stop
GET    /api/scheduler/status
GET    /api/scheduler/jobs
POST   /api/scheduler/trigger/{scraper_name}
GET    /api/scheduler/runs
GET    /api/scheduler/runs/{run_id}
GET    /api/scheduler/stats
```

### Licitaciones (URLs)
```
GET    /api/licitaciones/{id}/redirect
GET    /api/licitaciones/{id}/urls
POST   /api/licitaciones/{id}/resolve-url
GET    /api/licitaciones/stats/url-quality
POST   /api/licitaciones/deduplicate
```

---

## üìà Pr√≥ximos Pasos (Fase 3)

### Optimizaci√≥n

1) **Aumentar cobertura de PLIEGO a >80%**
   - Analizar por qu√© ~62 procesos no tienen URL p√∫blica
   - Verificar si hay otra vista p√∫blica (ComprasElectronicas.aspx)
   - Implementar fallback a detalle v√≠a API interna

2) **Performance**
   - Paralelizar scraping de m√∫ltiples fuentes
   - Optimizar tiempos de espera Selenium
   - Cache de p√°ginas HTML

3) **Dashboard de monitoreo**
   - UI para ver ejecuciones en tiempo real
   - Gr√°ficos de estad√≠sticas
   - Alertas de fallos

### Escalado

4) **Nuevas fuentes de Mendoza** (‚úÖ Implementado)
   - ‚úÖ AYSAM (Aguas Mendocinas)
   - ‚úÖ OSEP (Obra Social)
   - ‚úÖ UNCuyo (Universidad)
   - ‚úÖ Vialidad Provincial

5) **Otras provincias**
   - Buenos Aires (compras.gba.gob.ar)
   - CABA (buenosairescompras.gob.ar)
   - C√≥rdoba (compras.cba.gov.ar)
   - Santa Fe (santafe.gov.ar)

---

## üìö Documentaci√≥n

- `docs/PLAN_GRAN_SCRAPER.md` - Plan de arquitectura
- `docs/IMPLEMENTACION_SCRAPER_V2.md` - Gu√≠a t√©cnica
- `docs/README_SCRAPER_V2.md` - Documentaci√≥n usuario
- `docs/PROGRESO_SCRAPER_V2.md` - Progreso completo

---

*√öltima actualizaci√≥n: 2026-02-05*
